import requests
import pandas as pd

# Set up variables for the API parameters and pagination

# We decided to search for resources, which are artworks: a work of art in the Art Institute of Chicago collections

# We decided to use the following artworks' parameters:
   # id: unique identifier the artwork
   # title: the name of the artwork
   # has_not_been_viewed_much: whether an artwork has not been visited much on the website
   # has_multimedia_resources: whether the artwork has any associated microsites, digital
   # publication, or documents tagged as multimedia
   # has_educational_resources: whether the artwork has any documents tagged as educational
   # has_advanced_imaging: whether the artwork is enhanced with 3D models, 360 image
   # sequences, etc
   # date_start: first year associated with the creation of the work
   # date_end: last year associated with the creation of the work
   # medium_display: the substances or materials used in the creation of a work
   # is_public_domain: the work was created before copyrights existed or has left the copyright term
   # colorfulness: abstract measure of colorfulness
   # is_on_view: whether the artwork is on display
   # artwork_type_title: the kind of object or work
   # department_title: name of the curatiorial department that the work belongs to
   # artist_title: name of the preferred artist/sculture associated with the work
   

api = "https://api.artic.edu/api/v1/artworks"
parameters = {
    "fields": "id,title,has_not_been_viewed_much,has_multimedia_resources,has_educational_resources,"
              "has_advanced_imaging,date_start,date_end,medium_display,"
              "is_public_domain,colorfulness,is_on_view,artwork_type_title,"
              "department_title,artist_title",
    "limit": 100  # Maximum number of records per page
}

# We initialize an empty list to store the DataFrames
dfs = []

# We iterate over the desired pages and make API calls
for page in range(1, 20):  # Example: retrieve the first 2000 records (20 pages of 100 records each)
    parameters["page"] = page
    response = requests.get(api, params=parameters)
    data = response.json()
    dataframe = pd.DataFrame(data["data"])
    dfs.append(dataframe)

# We concatenate the DataFrames into a single DataFrame
df = pd.concat(dfs, ignore_index=True)

# Then, we print the first few rows of the DataFrame to check that everything works well
print(df.head())

# Finally, we save the dataFrame to a CSV file
df.to_csv("project_dataset.csv", index=False)
